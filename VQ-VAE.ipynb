{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68597a2c-b47c-44f4-9868-3db4a9f2298c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: opencv-python in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: lmdb in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.4.1)\n",
      "Requirement already satisfied: Pillow in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (10.3.0)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.66.4)\n",
      "Requirement already satisfied: PyYAML in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (6.0.1)\n",
      "Requirement already satisfied: pygame in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.5.2)\n",
      "Requirement already satisfied: scipy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: scikit-image in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.22.0)\n",
      "Requirement already satisfied: einops in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: seaborn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (3.9.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-image) (2024.5.22)\n",
      "Requirement already satisfied: packaging>=21 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-image) (24.0)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-image) (2.34.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (4.52.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy opencv-python lmdb Pillow tqdm PyYAML pygame scipy scikit-image einops seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241d0284-e5cb-416d-b7f1-d2da9ad574de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apex in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.9.10.dev0)\n",
      "Requirement already satisfied: velruse>=1.0.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from apex) (1.1.1)\n",
      "Requirement already satisfied: cryptacular in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from apex) (1.6.2)\n",
      "Requirement already satisfied: pyramid-mailer in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from apex) (0.15.1)\n",
      "Requirement already satisfied: zope.sqlalchemy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from apex) (3.1)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from apex) (2.31.0)\n",
      "Requirement already satisfied: wtforms-recaptcha in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from apex) (0.3.2)\n",
      "Requirement already satisfied: pyramid>1.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from apex) (2.0.2)\n",
      "Requirement already satisfied: wtforms in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from apex) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (69.5.1)\n",
      "Requirement already satisfied: plaster in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (1.1.2)\n",
      "Requirement already satisfied: plaster-pastedeploy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (1.0.1)\n",
      "Requirement already satisfied: translationstring>=0.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (1.4)\n",
      "Requirement already satisfied: zope.deprecation>=3.5.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (5.0)\n",
      "Requirement already satisfied: hupper>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (1.12.1)\n",
      "Requirement already satisfied: venusian>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (3.1.0)\n",
      "Requirement already satisfied: webob>=1.8.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (1.8.7)\n",
      "Requirement already satisfied: zope.interface>=3.8.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid>1.1.2->apex) (6.4.post2)\n",
      "Requirement already satisfied: requests-oauthlib in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from velruse>=1.0.3->apex) (2.0.0)\n",
      "Requirement already satisfied: python3-openid in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from velruse>=1.0.3->apex) (3.2.0)\n",
      "Requirement already satisfied: anykeystore in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from velruse>=1.0.3->apex) (0.2)\n",
      "Requirement already satisfied: pbkdf2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cryptacular->apex) (1.3)\n",
      "Requirement already satisfied: PasteDeploy>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from plaster-pastedeploy->pyramid>1.1.2->apex) (3.1.0)\n",
      "Requirement already satisfied: transaction in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid-mailer->apex) (4.0)\n",
      "Requirement already satisfied: repoze.sendmail>=4.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyramid-mailer->apex) (4.4.1)\n",
      "Requirement already satisfied: defusedxml in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->apex) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->apex) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->apex) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->apex) (2024.2.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.2.2)\n",
      "Requirement already satisfied: markupsafe in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from wtforms->apex) (2.1.5)\n",
      "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from zope.sqlalchemy->apex) (2.0.30)\n",
      "Requirement already satisfied: packaging in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from zope.sqlalchemy->apex) (24.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (3.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f0fae8-c77e-4d44-8e1d-b3203f4e156a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sconf in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.2.5)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: munch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sconf) (4.0.0)\n",
      "Requirement already satisfied: ruamel.yaml in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sconf) (0.18.6)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from ruamel.yaml->sconf) (0.2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sconf numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf0700b-e1b9-4e91-bc4d-7796e4f04d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.3.0%2Bcu118-cp39-cp39-linux_x86_64.whl (839.7 MB)\n",
      "\u001b[K     |███████████████▊                | 413.5 MB 135.1 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 839.7 MB 8.0 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.18.0%2Bcu118-cp39-cp39-linux_x86_64.whl (6.3 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.3.0%2Bcu118-cp39-cp39-linux_x86_64.whl (3.3 MB)\n",
      "Collecting nvidia-cublas-cu11==11.11.3.6\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "Collecting nvidia-curand-cu11==10.3.0.86\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.7.0.84\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.8.86\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Collecting nvidia-cusolver-cu11==11.4.1.48\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "Collecting filelock\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting nvidia-nccl-cu11==2.20.5\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl (142.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 142.9 MB 118.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "Collecting triton==2.3.0\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 168.1 MB 25 kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 65.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Requirement already satisfied: networkx in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.5.86\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.8.89\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "Collecting sympy\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 88.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 52.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: nvidia-cublas-cu11, mpmath, filelock, triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-cusolver-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, fsspec, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.13.1 fsspec-2024.2.0 mpmath-1.3.0 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.20.5 nvidia-nvtx-cu11-11.8.86 sympy-1.12 torch-2.3.0+cu118 torchaudio-2.3.0+cu118 torchvision-0.18.0+cu118 triton-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4c66b3-f6e2-4213-9171-50560b2e8e98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d23d100-c8f9-4927-a99e-d87aa75c4b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/VQ-Font\n"
     ]
    }
   ],
   "source": [
    "%cd /home/studio-lab-user/VQ-Font"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e12cff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "notebook_dir = Path().resolve().parent\n",
    "sys.path.append(str(notebook_dir.parent))  # the absolute path to the code\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from six.moves import xrange\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from model import content_enc_builder\n",
    "from model import dec_builder\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageFile\n",
    "from model.modules import weights_init\n",
    "import pprint\n",
    "import json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f867e7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "\n",
    "        self._num_embeddings = num_embeddings\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1 / self._num_embeddings, 1 / self._num_embeddings)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 传入的是图片经过encoder后的feature maps\n",
    "        # convert inputs from BCHW\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten input ->[BC HW]\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self._embedding.weight ** 2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) #得到编号\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized, perplexity, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc442a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.normal_()\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "\n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "\n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self._embedding.weight ** 2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "\n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "\n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                    (self._ema_cluster_size + self._epsilon)\n",
    "                    / (n + self._num_embeddings * self._epsilon) * n)\n",
    "\n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "\n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        loss = self._commitment_cost * e_latent_loss\n",
    "\n",
    "        # Straight Through Estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd437fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self._encoder = content_enc_builder(1,32,256)\n",
    "\n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = dec_builder(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x) #[B 256 16 16]\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59573f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombTrain_VQ_VAE_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    CombTrain_VQ_VAE_dataset, learn the laten codebook from content font. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform = None):\n",
    "        self.img_path = root\n",
    "        self.transform = transform\n",
    "        self.imgs = self.read_file(self.img_path)\n",
    "        # img = Image.open(self.imgs[0])\n",
    "        # img = self.transform(img)\n",
    "        # print(img.shape)\n",
    "\n",
    "\n",
    "    def read_file(self, path):\n",
    "        \"\"\"从文件夹中读取数据\"\"\"\n",
    "        files_list = os.listdir(path)\n",
    "        file_path_list = [os.path.join(path, img) for img in files_list]\n",
    "        file_path_list.sort()\n",
    "        return file_path_list\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.imgs[index]\n",
    "        #print(img_name[-5:-4])\n",
    "        img = Image.open(img_name)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img) #Tensor [C H W] [1 128 128]\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32bfe90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 iterations\n",
      "recon_error: 0.356\n",
      "perplexity: 15.902\n",
      "vq_loss: 663.667\n",
      "\n",
      "2000 iterations\n",
      "recon_error: 0.269\n",
      "perplexity: 17.211\n",
      "vq_loss: 9.531\n",
      "\n",
      "3000 iterations\n",
      "recon_error: 0.257\n",
      "perplexity: 36.981\n",
      "vq_loss: 0.605\n",
      "\n",
      "4000 iterations\n",
      "recon_error: 0.256\n",
      "perplexity: 67.838\n",
      "vq_loss: 0.108\n",
      "\n",
      "5000 iterations\n",
      "recon_error: 0.277\n",
      "perplexity: 63.957\n",
      "vq_loss: 1.672\n",
      "\n",
      "6000 iterations\n",
      "recon_error: 0.259\n",
      "perplexity: 63.240\n",
      "vq_loss: 0.059\n",
      "\n",
      "7000 iterations\n",
      "recon_error: 0.268\n",
      "perplexity: 53.918\n",
      "vq_loss: 1.835\n",
      "\n",
      "8000 iterations\n",
      "recon_error: 0.259\n",
      "perplexity: 57.178\n",
      "vq_loss: 0.674\n",
      "\n",
      "9000 iterations\n",
      "recon_error: 0.258\n",
      "perplexity: 50.371\n",
      "vq_loss: 0.383\n",
      "\n",
      "10000 iterations\n",
      "recon_error: 0.257\n",
      "perplexity: 37.814\n",
      "vq_loss: 0.078\n",
      "\n",
      "11000 iterations\n",
      "recon_error: 0.257\n",
      "perplexity: 33.217\n",
      "vq_loss: 0.016\n",
      "\n",
      "12000 iterations\n",
      "recon_error: 0.257\n",
      "perplexity: 32.843\n",
      "vq_loss: 0.018\n",
      "\n",
      "13000 iterations\n",
      "recon_error: 0.257\n",
      "perplexity: 26.221\n",
      "vq_loss: 0.060\n",
      "\n",
      "14000 iterations\n",
      "recon_error: 0.257\n",
      "perplexity: 32.422\n",
      "vq_loss: 0.093\n",
      "\n",
      "15000 iterations\n",
      "recon_error: 0.257\n",
      "perplexity: 32.351\n",
      "vq_loss: 0.074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_imgs_path = 'data/content/arial_115'\n",
    "tensorize_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "train_dataset = CombTrain_VQ_VAE_dataset(train_imgs_path, transform=tensorize_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=115, batch_sampler=None, drop_last=True, pin_memory=True, shuffle=True)\n",
    "\n",
    "num_training_updates = 15000\n",
    "\n",
    "embedding_dim = 256\n",
    "num_embeddings = 100\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "decay = 0\n",
    "\n",
    "learning_rate = 2e-4\n",
    "\n",
    "model = Model(num_embeddings, embedding_dim, commitment_cost, decay).to(device)\n",
    "model.apply(weights_init(\"xavier\"))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
    "\n",
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "train_vq_loss = []\n",
    "\n",
    "\n",
    "def val(model,validation_loader):\n",
    "    model.eval()\n",
    "\n",
    "    valid_originals = next(iter(validation_loader))\n",
    "    valid_originals = valid_originals.to(device)\n",
    "\n",
    "    vq_output_eval = model._encoder(valid_originals)\n",
    "    _, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "    valid_reconstructions = model._decoder(valid_quantize)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "for i in xrange(num_training_updates):\n",
    "    data = next(iter(train_loader))\n",
    "    train_data_variance = torch.var(data)\n",
    "    # print(train_data_variance)\n",
    "    # show(make_grid(data.cpu().data) )\n",
    "    # break\n",
    "    data = data - 0.5 # normalize to [-0.5, 0.5]\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    vq_loss, data_recon, perplexity = model(data)\n",
    "    # data_recon重构图像\n",
    "    # print(\"vq_loss\\n\",vq_loss)\n",
    "    recon_error = F.mse_loss(data_recon, data) / train_data_variance\n",
    "    loss = recon_error + vq_loss\n",
    "    # 重构损失更新encoder以及decoder,vq_loss用来更新embedding空间\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_res_recon_error.append(recon_error.item())\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "    train_vq_loss.append(vq_loss.item())\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print('%d iterations' % (i + 1))\n",
    "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-1000:]))\n",
    "        print('perplexity: %.3f' % np.mean(train_res_perplexity[-1000:]))\n",
    "        print('vq_loss: %.3f' % np.mean(train_vq_loss[-1000:]))\n",
    "        print()\n",
    "        # show(make_grid(data.cpu().data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b4632",
   "metadata": {},
   "source": [
    "## Old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b99062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_imgs_path = 'data/content/arial_115'\n",
    "tensorize_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "train_dataset = CombTrain_VQ_VAE_dataset(train_imgs_path, transform=tensorize_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=57, batch_sampler=None, drop_last=True, pin_memory=True, shuffle=True)\n",
    "\n",
    "num_training_updates = 20000\n",
    "\n",
    "embedding_dim = 128\n",
    "num_embeddings = 64\n",
    "\n",
    "commitment_cost = 0.2\n",
    "\n",
    "decay = 0\n",
    "\n",
    "learning_rate = 2e-4\n",
    "\n",
    "model = Model(num_embeddings, embedding_dim, commitment_cost, decay).to(device)\n",
    "model.apply(weights_init(\"xavier\"))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
    "\n",
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "train_vq_loss = []\n",
    "\n",
    "\n",
    "def val(model, validation_loader):\n",
    "    model.eval()\n",
    "\n",
    "    valid_originals = next(iter(validation_loader))\n",
    "    valid_originals = valid_originals.to(device)\n",
    "\n",
    "    vq_output_eval = model._encoder(valid_originals)\n",
    "    _, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "    valid_reconstructions = model._decoder(valid_quantize)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "for i in range(num_training_updates):\n",
    "    for data in train_loader:\n",
    "        train_data_variance = torch.var(data)\n",
    "        data = data - 0.5  # normalize to [-0.5, 0.5]\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        vq_loss, data_recon, perplexity = model(data)\n",
    "        recon_error = F.mse_loss(data_recon, data) / train_data_variance\n",
    "        loss = recon_error + vq_loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_res_recon_error.append(recon_error.item())\n",
    "        train_res_perplexity.append(perplexity.item())\n",
    "        train_vq_loss.append(vq_loss.item())\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print('%d iterations' % (i + 1))\n",
    "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-1000:]))\n",
    "        print('perplexity: %.3f' % np.mean(train_res_perplexity[-1000:]))\n",
    "        print('vq_loss: %.3f' % np.mean(train_vq_loss[-1000:]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf4ba5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs_path = 'data/content/arial_21'\n",
    "tensorize_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "val_dataset = CombTrain_VQ_VAE_dataset(val_imgs_path, transform=tensorize_transform)\n",
    "\n",
    "validation_loader = DataLoader(val_dataset, batch_size=7, batch_sampler=None, drop_last=True, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8453a418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..1.5].\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def val_(model,validation_loader):\n",
    "    model.eval()\n",
    "\n",
    "    valid_originals = next(iter(validation_loader))\n",
    "    valid_originals = valid_originals.to(device)\n",
    "\n",
    "    vq_output_eval = model._encoder(valid_originals)\n",
    "    _, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "    valid_reconstructions = model._decoder(valid_quantize)\n",
    "    return valid_originals, valid_reconstructions\n",
    "    \n",
    "org, recon_out = val_(model, validation_loader)\n",
    "show(make_grid((org+0.5).cpu().data), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "239b2606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..1.0112994].\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(make_grid((recon_out+0.5).cpu().data), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a5e55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'weight/VQ-VAE_chn_.pth')    #保存所有的网络参数\n",
    "torch.save(model.state_dict(),'weight/VQ-VAE_Parms_chn_.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec4963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim, num_embeddings, commitment_cost, decay = 256, 100, 0.25, 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(num_embeddings, embedding_dim, commitment_cost, decay).to(device)\n",
    "models = torch.load('weight/VQ-VAE_chn_.pth')\n",
    "encoder = models._encoder\n",
    "encoder.requires_gradq = False\n",
    "encoder.to(\"cpu\")\n",
    "#定义dataset\n",
    "class CombTrain_VQ_VAE_dataset(Dataset):\n",
    "    def __init__(self, root, transform = None):\n",
    "        self.img_path = root\n",
    "        self.transform = transform\n",
    "        self.imgs = self.read_file(self.img_path)\n",
    "        # img = Image.open(self.imgs[0])\n",
    "        # img = self.transform(img)\n",
    "        # print(img.shape)\n",
    "\n",
    "\n",
    "    def read_file(self, path):\n",
    "        \"\"\"从文件夹中读取数据\"\"\"\n",
    "        files_list = os.listdir(path)\n",
    "        file_path_list = [os.path.join(path, img) for img in files_list]\n",
    "        file_path_list.sort()\n",
    "        return file_path_list\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.imgs[index]\n",
    "        img = Image.open(img_name)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img) #Tensor [C H W] [1 128 128]\n",
    "        ret =(img_name, \n",
    "              img\n",
    "        )\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_imgs_path = 'data/content/arial_136'\n",
    "tensorize_transform = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                          transforms.ToTensor()])\n",
    "\n",
    "batch = 136 # all content imgs\n",
    "\n",
    "sim_dataset = CombTrain_VQ_VAE_dataset(train_imgs_path, transform=tensorize_transform)\n",
    "\n",
    "sim_loader = DataLoader(sim_dataset, batch_size=batch, batch_sampler=None, drop_last=False, pin_memory=True)\n",
    "\n",
    "similarity = []\n",
    "\n",
    "def CosineSimilarity(tensor_1, tensor_2):\n",
    "    normalized_tensor_1 = tensor_1 / tensor_1.norm(dim=-1, keepdim=True)\n",
    "    normalized_tensor_2 = tensor_2 / tensor_2.norm(dim=-1, keepdim=True)\n",
    "    return (normalized_tensor_1 * normalized_tensor_2).sum(dim=-1)\n",
    "\n",
    "\n",
    "while True:\n",
    "    data = next(iter(sim_loader))\n",
    "    img_name = data[0]\n",
    "    img_tensor = data[1]\n",
    "    img_tensor = img_tensor - 0.5 # normalize to [-0.5, 0.5]\n",
    "    img_tensor = img_tensor.to(\"cpu\")\n",
    "    \n",
    "    #得到了conten的feature\n",
    "    content_feature = encoder(img_tensor)\n",
    "    # print(content_feature.shape)\n",
    "    vector = content_feature.view(content_feature.shape[0], -1)\n",
    "    # print(vector.shape)\n",
    "    \n",
    "    sim_all = {}\n",
    "    for i in range(0,batch):\n",
    "        char_i = hex(ord(img_name[i].split('_')[2]))[2:].upper()\n",
    "        dict_sim_i = {char_i:{}}\n",
    "        for j in range(0,batch):\n",
    "            char_j = hex(ord(img_name[j].split('_')[2]))[2:].upper()\n",
    "            similarity = CosineSimilarity(vector[i],vector[j])\n",
    "            if i==j:\n",
    "                similarity=1.0\n",
    "            sim_i2j = {char_j:float(similarity)}\n",
    "            dict_sim_i[char_i].update(sim_i2j)\n",
    "        sim_all.update(dict_sim_i)\n",
    "\n",
    "        \n",
    "    dict_json=json.dumps(sim_all)#转化为json格式文件\n",
    "\n",
    "    #将json文件保存为.json格式文件\n",
    "    with open('weight/all_char_similarity_unicode.json','w+', encoding='utf-8') as file:\n",
    "        file.write(dict_json)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9318891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'61': 0.04263447970151901, '62': 0.536594033241272, '63': 0.0761694461107254, '64': 0.4300118684768677, '65': 0.022931458428502083, '66': 0.09344466030597687, '67': 0.37584951519966125, '68': 0.47559216618537903, '69': 0.14127491414546967, '6A': 0.06462696194648743, '6B': 0.5145952701568604, '6C': 0.12115943431854248, '6D': -0.04853231832385063, '6E': 0.19519658386707306, '6F': 0.034251026809215546, '70': 0.2652084529399872, '71': 0.22408391535282135, '72': 0.0075524188578128815, '73': 0.06608428061008453, '74': 0.17567986249923706, '75': 0.15435707569122314, '76': 0.07093852758407593, '77': -0.03914494067430496, '78': 0.07406730949878693, '79': 0.2677600085735321, '7A': 0.01658780686557293, '430': 0.04263447970151901, '431': 0.3601335287094116, '432': 0.07499417662620544, '433': -0.01811268925666809, '434': 0.12491495907306671, '435': 0.022931458428502083, '436': -0.09508385509252548, '437': 0.025590412318706512, '438': 0.1606648862361908, '439': 0.32673853635787964, '43A': 0.18214502930641174, '43B': 0.1607888638973236, '43C': -0.08814536035060883, '43D': 0.1946144700050354, '43E': 0.034251026809215546, '43F': 0.19783920049667358, '440': 0.2652084529399872, '441': 0.0761694461107254, '442': 0.021747298538684845, '443': 0.2677600085735321, '444': -0.0568210631608963, '445': 0.07406730949878693, '446': 0.14041785895824432, '447': 0.10398691892623901, '448': -0.13005945086479187, '449': -0.08403189480304718, '44A': 0.009718016721308231, '44B': -0.11148259043693542, '44C': 0.11586451530456543, '44D': 0.027674131095409393, '44E': -0.16903768479824066, '44F': 0.08237317949533463, '451': 0.3424363136291504, '456': 0.14127491414546967, '493': 0.004637149162590504, '49B': 0.23159238696098328, '4A3': 0.2771860957145691, '4AF': 0.25264960527420044, '4B1': 0.1882752776145935, '4BB': 0.47559216618537903, '4D9': 0.05269278585910797, '4E9': 0.023482628166675568, '41': 0.04989314079284668, '42': 0.3355467915534973, '43': 0.359060674905777, '44': 0.28533217310905457, '45': 0.4088365435600281, '46': 0.30126893520355225, '47': 0.349799245595932, '48': 0.03180333971977234, '49': 0.09831098467111588, '4A': 0.40650463104248047, '4B': 0.09432338178157806, '4C': 1.0, '4D': 0.08272776007652283, '4E': 0.06865722686052322, '4F': 0.33920222520828247, '50': 0.13126127421855927, '51': 0.24377863109111786, '52': 0.0388249009847641, '53': 0.36662447452545166, '54': 0.16175195574760437, '55': 0.33777862787246704, '56': 0.19904430210590363, '57': 0.13778027892112732, '58': 0.15075470507144928, '59': 0.16241878271102905, '5A': 0.46519580483436584, '401': 0.2654253840446472, '406': 0.09831098467111588, '410': 0.04989314079284668, '411': 0.40591108798980713, '412': 0.3355467915534973, '413': 0.4711777865886688, '414': 0.06416478753089905, '415': 0.4088365435600281, '416': -0.02194078639149666, '417': 0.3780900239944458, '418': 0.02555793896317482, '419': -0.05045431852340698, '41A': 0.4469897747039795, '41B': 0.3938390016555786, '41C': 0.08272776007652283, '41D': 0.03180333971977234, '41E': 0.33920222520828247, '41F': 0.08131840825080872, '420': 0.13126127421855927, '421': 0.359060674905777, '422': 0.16175195574760437, '423': 0.20891499519348145, '424': 0.018492523580789566, '425': 0.15075470507144928, '426': -0.1738266944885254, '427': 0.11225923895835876, '428': 0.28763100504875183, '429': -0.14765328168869019, '42A': 0.6003091335296631, '42B': 0.1541510820388794, '42C': 0.43988096714019775, '42D': 0.31094682216644287, '42E': 0.3312833607196808, '42F': 0.1365855634212494, '492': 0.362619012594223, '49A': 0.19025030732154846, '4A2': -0.0712399035692215, '4AE': 0.20353621244430542, '4B0': 0.2099384367465973, '4BA': 0.12375445663928986, '4D8': 0.3176502287387848, '4E8': 0.3107427656650543}\n"
     ]
    }
   ],
   "source": [
    "with open('weight/all_char_similarity_unicode.json','r+', encoding='utf-8') as file:\n",
    "    content=file.read()\n",
    "    \n",
    "content=json.loads(content)#将json格式文件转化为python的字典文件\n",
    "print(content['4C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8d48c-75e7-4797-808b-f09f7367420b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
